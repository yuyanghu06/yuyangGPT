{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68041ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pypdf import PdfReader\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a76563",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"E:\\\\YuyangGPT\\\\dataset\\\\raw_data\"      # folder with .pdf and .docx files\n",
    "OUTPUT_FILE = \"E:\\\\YuyangGPT\\\\dataset\\\\cleaned_data\\\\all_data.jsonl\"  # combined output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f26f2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    pages = []\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            pages.append(text)\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "def extract_text_from_docx(path):\n",
    "    doc = Document(path)\n",
    "    paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "    return \"\\n\".join(paragraphs)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\r\", \"\\n\")\n",
    "    text = \"\\n\".join(\n",
    "        line.strip()\n",
    "        for line in text.split(\"\\n\")\n",
    "        if line.strip()\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, min_len=50):\n",
    "    chunks = []\n",
    "    for paragraph in text.split(\"\\n\"):\n",
    "        if len(paragraph) >= min_len:\n",
    "            chunks.append(paragraph)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefd0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2_8 Interview Prep.docx...\n",
      "Processing BAC Consulting Presentation Notes.docx...\n",
      "Processing Brochure.docx...\n",
      "Processing Civics Politics Writeup.docx...\n",
      "Processing College Leader Interview Prep.docx...\n",
      "Processing Danish Visa Cover Letter.docx...\n",
      "Processing earth day speech!.docx...\n",
      "Processing EASU script.docx...\n",
      "Processing empire book.docx...\n",
      "Processing English Narrative.docx...\n",
      "Processing Exile and Migration Final Project.docx...\n",
      "Processing Final Project Proposal.docx...\n",
      "Processing First Year Seminar Notes.docx...\n",
      "Processing NYU Shanghai Personal Statement.docx...\n",
      "Processing Oedipus Rex.docx...\n",
      "Processing Progression 1 .docx...\n",
      "Processing Recitation 5.docx...\n",
      "Processing Recitation 7.docx...\n",
      "Processing recitation assignment 9.docx...\n",
      "Processing Samantha LinkedIn Message.docx...\n",
      "Processing Script Outline.docx...\n",
      "Processing Seminar Transcript Assignment.docx...\n",
      "Processing Summer AI Integration Project.docx...\n",
      "Processing task_5.docx...\n",
      "Processing Texts & Ideas Presentation.docx...\n",
      "Processing Texts and Ideas Final Essay.docx...\n",
      "Processing What Really Matters Week 10.docx...\n",
      "Processing What Really Matters Week 11.docx...\n",
      "Processing What Really Matters Week 12.docx...\n",
      "Processing What Really Matters Week 2.docx...\n",
      "Processing What Really Matters Week 3.docx...\n",
      "Processing What Really Matters Week 4.docx...\n",
      "Processing What Really Matters Week 5.docx...\n",
      "Processing What Really Matters Week 6.docx...\n",
      "Processing What Really Matters Week 7.docx...\n",
      "Processing What Really Matters Week 8.docx...\n",
      "Processing What Really Matters Week 9.docx...\n",
      "Processing Writing as Inquiry Scene Homework.docx...\n",
      "Processing Yuyang Hu - Final Progression 2 essay .docx...\n",
      "Processing Yuyang Hu Cover Letter Dropbox.docx...\n",
      "Processing Yuyang Hu Cover Letter.docx...\n",
      "Processing Yuyang Hu Resume.docx...\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    for filename in os.listdir(INPUT_DIR):\n",
    "        if not filename.endswith((\".pdf\", \".docx\")):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(INPUT_DIR, filename)\n",
    "        print(f\"Processing {filename}...\")\n",
    "\n",
    "        try:\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                text = extract_text_from_pdf(path)\n",
    "            else:\n",
    "                text = extract_text_from_docx(path)\n",
    "\n",
    "            text = clean_text(text)\n",
    "            chunks = chunk_text(text)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                record = {\"text\": chunk}\n",
    "                out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69742a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
