{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0038d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "MODEL_DIR = \"E:\\\\YuyangGPT\\\\models\\\\minilm-custom-eos\"\n",
    "CHUNK_SIZE = 128\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer + embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModel.from_pretrained(MODEL_DIR).to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ff469",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39daa345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkTransformer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Learnable summary token\n",
    "        self.summary_token = nn.Parameter(\n",
    "            torch.randn(1, 1, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        \"\"\"\n",
    "        hidden_states: [B, T, d]\n",
    "        attention_mask: [B, T]\n",
    "        \"\"\"\n",
    "\n",
    "        B, T, d = hidden_states.shape\n",
    "\n",
    "        # Expand summary token for batch\n",
    "        summary = self.summary_token.expand(B, 1, d)\n",
    "\n",
    "        # Prepend summary token\n",
    "        x = torch.cat([summary, hidden_states], dim=1)  # [B, T+1, d]\n",
    "\n",
    "        # Build attention mask (1 = keep, 0 = mask)\n",
    "        summary_mask = torch.ones(B, 1, device=attention_mask.device)\n",
    "        attn_mask = torch.cat([summary_mask, attention_mask], dim=1)\n",
    "\n",
    "        # TransformerEncoder uses True = masked\n",
    "        key_padding_mask = attn_mask == 0\n",
    "\n",
    "        out = self.encoder(\n",
    "            x,\n",
    "            src_key_padding_mask=key_padding_mask\n",
    "        )\n",
    "\n",
    "        # Return summary token output\n",
    "        return out[:, 0]  # [B, d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongTermMemory(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        num_latents=8,\n",
    "        num_layers=4,\n",
    "        num_heads=4\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Learned latent tokens\n",
    "        self.latents = nn.Parameter(\n",
    "            torch.randn(1, num_latents, hidden_size)\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, summaries, attention_mask=None):\n",
    "        \"\"\"\n",
    "        summaries: [B, T, d]\n",
    "        returns:   [B, K, d]\n",
    "        \"\"\"\n",
    "\n",
    "        B = summaries.size(0)\n",
    "\n",
    "        latents = self.latents.expand(B, -1, -1)\n",
    "\n",
    "        # Concatenate: latents attend to summaries\n",
    "        x = torch.cat([latents, summaries], dim=1)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            latent_mask = torch.ones(\n",
    "                B,\n",
    "                latents.size(1),\n",
    "                device=attention_mask.device\n",
    "            )\n",
    "            attn_mask = torch.cat([latent_mask, attention_mask], dim=1)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        out = self.encoder(x, src_key_padding_mask=(attn_mask == 0 if attn_mask is not None else None))\n",
    "\n",
    "        # Return only latent outputs\n",
    "        return out[:, :latents.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8307e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTMPredictorHead(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, latents):\n",
    "        \"\"\"\n",
    "        latents: [B, K, d]\n",
    "        returns: [B, K, d]\n",
    "        \"\"\"\n",
    "        return self.net(latents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be6c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stm = ChunkTransformer(\n",
    "    hidden_size=model.config.hidden_size,\n",
    "    num_layers=2,\n",
    "    num_heads=4\n",
    ").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_tokens(input_ids, attention_mask, chunk_size=128):\n",
    "    chunks = []\n",
    "\n",
    "    seq_len = input_ids.size(1)\n",
    "\n",
    "    for start in range(0, seq_len, chunk_size):\n",
    "        end = start + chunk_size\n",
    "\n",
    "        chunk_ids = input_ids[:, start:end]\n",
    "        chunk_mask = attention_mask[:, start:end]\n",
    "\n",
    "        if chunk_ids.size(1) == 0:\n",
    "            continue\n",
    "\n",
    "        chunks.append({\n",
    "            \"input_ids\": chunk_ids,\n",
    "            \"attention_mask\": chunk_mask\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def encode_chunks(chunks, stm_model):\n",
    "    summaries = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for chunk in chunks:\n",
    "            outputs = model(\n",
    "                input_ids=chunk[\"input_ids\"],\n",
    "                attention_mask=chunk[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "            # Token-level hidden states\n",
    "            h = outputs.last_hidden_state  # [1, T, d]\n",
    "\n",
    "            summary = stm_model(\n",
    "                hidden_states=h,\n",
    "                attention_mask=chunk[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "            summaries.append(summary.squeeze(0))  # [d]\n",
    "\n",
    "    return torch.stack(summaries)  # [num_chunks, d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ddee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STMTrainer(nn.Module):\n",
    "    def __init__(self, stm, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.stm = stm\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        \"\"\"\n",
    "        hidden_states: [B, T, d]  (from MiniLM)\n",
    "        attention_mask: [B, T]\n",
    "        \"\"\"\n",
    "        summary = self.stm(hidden_states, attention_mask)  # [B, d]\n",
    "        logits = self.lm_head(summary)                      # [B, vocab]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459191f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stm_trainer = STMTrainer(\n",
    "    stm=stm,\n",
    "    hidden_size=model.config.hidden_size,\n",
    "    vocab_size=len(tokenizer)\n",
    ").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(stm_trainer.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16183f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_chunks(chunks, model, stm_trainer):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(len(chunks) - 1):\n",
    "        # Encode current chunk\n",
    "        outputs = model(\n",
    "            input_ids=chunks[i][\"input_ids\"],\n",
    "            attention_mask=chunks[i][\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "        h = outputs.last_hidden_state  # [1, T, d]\n",
    "\n",
    "        # Predict next chunk's first token\n",
    "        logits = stm_trainer(\n",
    "            hidden_states=h,\n",
    "            attention_mask=chunks[i][\"attention_mask\"]\n",
    "        )  # [1, vocab]\n",
    "\n",
    "        target = chunks[i + 1][\"input_ids\"][:, 0]  # [1]\n",
    "\n",
    "        loss = loss_fn(logits, target)\n",
    "        loss.backward()\n",
    "        for name, p in stm_trainer.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                print(name, p.grad.norm())\n",
    "                break\n",
    "        assert loss.requires_grad\n",
    "        assert stm_trainer.lm_head.weight.grad is not None\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / (len(chunks) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88189535",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "stm_dir = \"E:\\YuyangGPT\\models\\stms\\stm_3\"\n",
    "JSONL_PATH = \"E:\\\\YuyangGPT\\\\dataset\\\\cleaned_data\\\\train_tokenized_discord_messages.jsonl\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78941edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cdaa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    n_docs = 0\n",
    "\n",
    "    for sample in load_jsonl(JSONL_PATH):\n",
    "        # Convert lists → tensors\n",
    "        input_ids = torch.tensor(\n",
    "            sample[\"input_ids\"],\n",
    "            dtype=torch.long,\n",
    "            device=DEVICE\n",
    "        ).unsqueeze(0)  # [1, T]\n",
    "\n",
    "        attention_mask = torch.tensor(\n",
    "            sample[\"attention_mask\"],\n",
    "            dtype=torch.long,\n",
    "            device=DEVICE\n",
    "        ).unsqueeze(0)  # [1, T]\n",
    "\n",
    "        # Skip short sequences\n",
    "        if input_ids.size(1) <= CHUNK_SIZE:\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_tokens(input_ids, attention_mask, CHUNK_SIZE)\n",
    "\n",
    "        if len(chunks) < 2:\n",
    "            continue\n",
    "\n",
    "        loss = train_on_chunks(chunks, model, stm_trainer)\n",
    "\n",
    "        total_loss += loss\n",
    "        n_docs += 1\n",
    "\n",
    "    avg_loss = total_loss / max(n_docs, 1)\n",
    "    print(f\"Epoch {epoch} | STM loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ab0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"stm_state_dict\": stm.state_dict(),\n",
    "        \"stm_config\": {\n",
    "            \"hidden_size\": model.config.hidden_size,\n",
    "            \"num_layers\": 2,\n",
    "            \"num_heads\": 4\n",
    "        }\n",
    "    },\n",
    "    f\"{stm_dir}/stm_checkpoint.pt\"\n",
    ")\n",
    "torch.save(\n",
    "    stm_trainer.state_dict(),\n",
    "    f\"{stm_dir}/stm_trainer_checkpoint.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf7cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ltm_targets(summaries, K):\n",
    "    \"\"\"\n",
    "    summaries: [B, T, d]\n",
    "    \"\"\"\n",
    "    context = summaries[:, :-K]        # [B, T-K, d]\n",
    "    targets = summaries[:, -K:]         # [B, K, d]\n",
    "    return context, targets\n",
    "\n",
    "def train_ltm_step(\n",
    "    summaries,\n",
    "    ltm,\n",
    "    predictor,\n",
    "    optimizer,\n",
    "    K\n",
    "):\n",
    "    \"\"\"\n",
    "    summaries: [B, T, d]\n",
    "    \"\"\"\n",
    "\n",
    "    context, targets = make_ltm_targets(summaries, K)\n",
    "\n",
    "    latents = ltm(context)              # [B, K, d]\n",
    "    preds = predictor(latents)          # [B, K, d]\n",
    "\n",
    "    loss = F.mse_loss(preds, targets)\n",
    "    print(\"context\", context.shape, context.dtype, context.device)\n",
    "    print(\"targets\", targets.shape, targets.dtype, targets.device)\n",
    "    print(\"latents requires_grad:\", latents.requires_grad)\n",
    "    print(\"preds   requires_grad:\", preds.requires_grad)\n",
    "    print(\"loss    requires_grad:\", loss.requires_grad, \"loss:\", loss.item())\n",
    "\n",
    "    # sanity: are your targets all zeros?\n",
    "    print(\"targets abs mean:\", targets.abs().mean().item(), \"std:\", targets.std().item())\n",
    "    print(\"preds   abs mean:\", preds.abs().mean().item(), \"std:\", preds.std().item())\n",
    "    print(\"mse raw:\", ((preds - targets) ** 2).mean().item())   \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for name, p in predictor.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            print(name, p.grad.norm())\n",
    "            break\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "stm.load_state_dict(torch.load(f\"{stm_dir}/stm_checkpoint.pt\")[\"stm_state_dict\"])\n",
    "stm_trainer.load_state_dict(torch.load(f\"{stm_dir}/stm_trainer_checkpoint.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a561b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2 # number of latents\n",
    "ltm = LongTermMemory(\n",
    "    hidden_size=model.config.hidden_size,\n",
    "    num_latents=K,\n",
    "    num_layers=4,\n",
    "    num_heads=4\n",
    ").to(DEVICE)\n",
    "\n",
    "ltm_predictor = LTMPredictorHead(\n",
    "    hidden_size=model.config.hidden_size\n",
    ").to(DEVICE)\n",
    "\n",
    "ltm_optimizer = torch.optim.AdamW(\n",
    "    list(ltm.parameters()) + list(ltm_predictor.parameters()),\n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    n_docs = 0\n",
    "\n",
    "    for sample in load_jsonl(JSONL_PATH):\n",
    "        # Convert lists → tensors\n",
    "        input_ids = torch.tensor(\n",
    "            sample[\"input_ids\"],\n",
    "            dtype=torch.long,\n",
    "            device=DEVICE\n",
    "        ).unsqueeze(0)  # [1, T]\n",
    "        # print(\"input_ids\", input_ids.shape, input_ids.dtype, input_ids.device)\n",
    "\n",
    "        attention_mask = torch.tensor(\n",
    "            sample[\"attention_mask\"],\n",
    "            dtype=torch.long,\n",
    "            device=DEVICE\n",
    "        ).unsqueeze(0)  # [1, T]\n",
    "        # print(\"attention_mask\", attention_mask.shape, attention_mask.dtype, attention_mask.device)\n",
    "\n",
    "        # Skip short sequences\n",
    "        if input_ids.size(1) <= CHUNK_SIZE:\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_tokens(input_ids, attention_mask, CHUNK_SIZE)\n",
    "\n",
    "        if len(chunks) < K + 1:\n",
    "            print(\"Skipping sample, not enough chunks:\", len(chunks))\n",
    "            continue\n",
    "        summaries = encode_chunks(chunks, stm).unsqueeze(0)  # [1, num_chunks, d]\n",
    "        print(\"summaries\", summaries.shape, summaries.dtype, summaries.device)\n",
    "        loss = train_ltm_step(\n",
    "            summaries,\n",
    "            ltm,\n",
    "            ltm_predictor,\n",
    "            ltm_optimizer,\n",
    "            K\n",
    "        )\n",
    "        total_loss += loss\n",
    "        n_docs += 1\n",
    "\n",
    "    avg_loss = total_loss / max(n_docs, 1)\n",
    "    print(f\"Epoch {epoch} | LTM loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46720fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm_dir = \"E:\\YuyangGPT\\models\\ltms\\ltm_1\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"ltm_state_dict\": ltm.state_dict(),\n",
    "        \"ltm_config\": {\n",
    "            \"hidden_size\": model.config.hidden_size,\n",
    "            \"num_latents\": K,\n",
    "            \"num_layers\": 4,\n",
    "            \"num_heads\": 4\n",
    "        },\n",
    "        \"predictor_state_dict\": ltm_predictor.state_dict()\n",
    "    },\n",
    "    f\"{ltm_dir}/ltm_checkpoint.pt\"\n",
    ")\n",
    "\n",
    "torch.save(\n",
    "    ltm_predictor.state_dict(),\n",
    "    f\"{ltm_dir}/ltm_predictor_checkpoint.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "This is a test of YuyangGPT v1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cca0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71d4aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_tokens(inputs[\"input_ids\"], inputs[\"attention_mask\"], chunk_size=128)\n",
    "\n",
    "summaries = encode_chunks(chunks, stm)\n",
    "\n",
    "# Add batch dimension for LTM\n",
    "summaries_batched = summaries.unsqueeze(0)  # [1, num_chunks, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d5c9352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ITERATION 1 =====\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     20\u001b[39m chunks = chunk_tokens(\n\u001b[32m     21\u001b[39m     token_embeddings,\n\u001b[32m     22\u001b[39m     current_attention_mask,\n\u001b[32m     23\u001b[39m     CHUNK_SIZE\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# ---- STM pass ----\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m summaries = \u001b[43mencode_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m summaries_batched = summaries.unsqueeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# [1, C, d]\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSummaries shape:\u001b[39m\u001b[33m\"\u001b[39m, summaries_batched.shape)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mencode_chunks\u001b[39m\u001b[34m(chunks, stm_model)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m         \u001b[38;5;66;03m# Token-level hidden states\u001b[39;00m\n\u001b[32m     33\u001b[39m         h = outputs.last_hidden_state  \u001b[38;5;66;03m# [1, T, d]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\YuyangGPT\\trainer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\YuyangGPT\\trainer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\YuyangGPT\\trainer\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:917\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    914\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    915\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m batch_size, seq_length = input_shape\n\u001b[32m    918\u001b[39m device = input_ids.device \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds.device\n\u001b[32m    920\u001b[39m past_key_values_length = \u001b[32m0\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "N = 3          # number of memory refinement steps\n",
    "CHUNK_SIZE = 128\n",
    "\n",
    "# Initialize inputs\n",
    "current_input_ids = inputs[\"input_ids\"]\n",
    "current_attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "for step in range(N):\n",
    "    print(f\"\\n===== ITERATION {step + 1} =====\")\n",
    "\n",
    "    # ---- Embed tokens ----\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=current_input_ids,\n",
    "            attention_mask=current_attention_mask\n",
    "        )\n",
    "    token_embeddings = outputs.last_hidden_state  # [1, T, d]\n",
    "\n",
    "    # ---- Chunk embeddings ----\n",
    "    chunks = chunk_tokens(\n",
    "        token_embeddings,\n",
    "        current_attention_mask,\n",
    "        CHUNK_SIZE\n",
    "    )\n",
    "\n",
    "    # ---- STM pass ----\n",
    "    summaries = encode_chunks(chunks, stm)\n",
    "    summaries_batched = summaries.unsqueeze(0)  # [1, C, d]\n",
    "\n",
    "    print(\"Summaries shape:\", summaries_batched.shape)\n",
    "\n",
    "    # ---- LTM pass ----\n",
    "    with torch.no_grad():\n",
    "        ltm_latents = ltm(summaries_batched)  # [1, K, d]\n",
    "\n",
    "    print(\"LTM latents shape:\", ltm_latents.shape)\n",
    "\n",
    "    # ---- Decode LTM latents into tokens ----\n",
    "    ltm_logits = stm_trainer.lm_head(ltm_latents)   # [1, K, vocab]\n",
    "    ltm_tokens = torch.argmax(ltm_logits, dim=-1)  # [1, K]\n",
    "\n",
    "    decoded_text = tokenizer.decode(\n",
    "        ltm_tokens[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(\"LTM decoded text:\", decoded_text)\n",
    "\n",
    "    # ---- Append tokens to input ----\n",
    "    current_input_ids = torch.cat(\n",
    "        [current_input_ids, ltm_tokens],\n",
    "        dim=1\n",
    "    )  # [1, T + K]\n",
    "\n",
    "    current_attention_mask = torch.ones_like(current_input_ids)\n",
    "\n",
    "    print(\"New input length:\", current_input_ids.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9a7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
