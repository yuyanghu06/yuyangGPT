{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0038d726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\YuyangGPT\\trainer\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fcce78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a6cf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30523, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define constants\n",
    "MODEL_DIR = \"E:\\\\YuyangGPT\\\\models\\\\minilm-custom-eos\"\n",
    "CHUNK_SIZE = 128\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer + embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModel.from_pretrained(MODEL_DIR).to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290ff469",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80db1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_tokens(input_ids, attention_mask, chunk_size=128):\n",
    "    chunks = []\n",
    "\n",
    "    seq_len = input_ids.size(1)\n",
    "\n",
    "    for start in range(0, seq_len, chunk_size):\n",
    "        end = start + chunk_size\n",
    "\n",
    "        chunk_ids = input_ids[:, start:end]\n",
    "        chunk_mask = attention_mask[:, start:end]\n",
    "\n",
    "        if chunk_ids.size(1) == 0:\n",
    "            continue\n",
    "\n",
    "        chunks.append({\n",
    "            \"input_ids\": chunk_ids,\n",
    "            \"attention_mask\": chunk_mask\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def encode_chunks(chunks, stm_model):\n",
    "    summaries = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for chunk in chunks:\n",
    "            outputs = model(\n",
    "                input_ids=chunk[\"input_ids\"],\n",
    "                attention_mask=chunk[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "            # Token-level hidden states\n",
    "            h = outputs.last_hidden_state  # [1, T, d]\n",
    "\n",
    "            summary = stm_model(\n",
    "                hidden_states=h,\n",
    "                attention_mask=chunk[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "            summaries.append(summary.squeeze(0))  # [d]\n",
    "\n",
    "    return torch.stack(summaries)  # [num_chunks, d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39daa345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkTransformer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Learnable summary token\n",
    "        self.summary_token = nn.Parameter(\n",
    "            torch.randn(1, 1, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        \"\"\"\n",
    "        hidden_states: [B, T, d]\n",
    "        attention_mask: [B, T]\n",
    "        \"\"\"\n",
    "\n",
    "        B, T, d = hidden_states.shape\n",
    "\n",
    "        # Expand summary token for batch\n",
    "        summary = self.summary_token.expand(B, 1, d)\n",
    "\n",
    "        # Prepend summary token\n",
    "        x = torch.cat([summary, hidden_states], dim=1)  # [B, T+1, d]\n",
    "\n",
    "        # Build attention mask (1 = keep, 0 = mask)\n",
    "        summary_mask = torch.ones(B, 1, device=attention_mask.device)\n",
    "        attn_mask = torch.cat([summary_mask, attention_mask], dim=1)\n",
    "\n",
    "        # TransformerEncoder uses True = masked\n",
    "        key_padding_mask = attn_mask == 0\n",
    "\n",
    "        out = self.encoder(\n",
    "            x,\n",
    "            src_key_padding_mask=key_padding_mask\n",
    "        )\n",
    "\n",
    "        # Return summary token output\n",
    "        return out[:, 0]  # [B, d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d30ddee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STMTrainer(nn.Module):\n",
    "    def __init__(self, stm, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.stm = stm\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        \"\"\"\n",
    "        hidden_states: [B, T, d]  (from MiniLM)\n",
    "        attention_mask: [B, T]\n",
    "        \"\"\"\n",
    "        summary = self.stm(hidden_states, attention_mask)  # [B, d]\n",
    "        logits = self.lm_head(summary)                      # [B, vocab]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad414abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stm = ChunkTransformer(\n",
    "    hidden_size=model.config.hidden_size,\n",
    "    num_layers=2,\n",
    "    num_heads=4\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "459191f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stm_trainer = STMTrainer(\n",
    "    stm=stm,\n",
    "    hidden_size=model.config.hidden_size,\n",
    "    vocab_size=len(tokenizer)\n",
    ").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ed51531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTIONAL, LOAD STM AND STM_TRAINER PARAM DICTs\n",
    "stm_dir = \"E:\\YuyangGPT\\models\\stms\\stm_3\"\n",
    "\n",
    "# ---- Load STM ----\n",
    "stm_ckpt = torch.load(\n",
    "    f\"{stm_dir}/stm_checkpoint.pt\",\n",
    "    map_location=DEVICE\n",
    ")\n",
    "\n",
    "stm.load_state_dict(stm_ckpt[\"stm_state_dict\"])\n",
    "\n",
    "# ---- Load STM trainer ----\n",
    "trainer_state = torch.load(\n",
    "    f\"{stm_dir}/stm_trainer_checkpoint.pt\",\n",
    "    map_location=DEVICE\n",
    ")\n",
    "\n",
    "stm_trainer.load_state_dict(trainer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(stm_trainer.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16183f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_chunks(chunks, model, stm_trainer):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(len(chunks) - 1):\n",
    "        # Encode current chunk\n",
    "        outputs = model(\n",
    "            input_ids=chunks[i][\"input_ids\"],\n",
    "            attention_mask=chunks[i][\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "        h = outputs.last_hidden_state  # [1, T, d]\n",
    "\n",
    "        # Predict next chunk's first token\n",
    "        logits = stm_trainer(\n",
    "            hidden_states=h,\n",
    "            attention_mask=chunks[i][\"attention_mask\"]\n",
    "        )  # [1, vocab]\n",
    "\n",
    "        target = chunks[i + 1][\"input_ids\"][:, 0]  # [1]\n",
    "\n",
    "        loss = loss_fn(logits, target)\n",
    "        loss.backward()\n",
    "        for name, p in stm_trainer.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                print(name, p.grad.norm())\n",
    "                break\n",
    "        assert loss.requires_grad\n",
    "        assert stm_trainer.lm_head.weight.grad is not None\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / (len(chunks) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88189535",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "stm_dir = \"E:\\YuyangGPT\\models\\stms\\stm_3\"\n",
    "JSONL_PATH = \"E:\\\\YuyangGPT\\\\dataset\\\\cleaned_data\\\\train_tokenized_discord_messages.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cdaa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    n_docs = 0\n",
    "\n",
    "    for sample in load_jsonl(JSONL_PATH):\n",
    "        # Convert lists → tensors\n",
    "        input_ids = torch.tensor(\n",
    "            sample[\"input_ids\"],\n",
    "            dtype=torch.long,\n",
    "            device=DEVICE\n",
    "        ).unsqueeze(0)  # [1, T]\n",
    "\n",
    "        attention_mask = torch.tensor(\n",
    "            sample[\"attention_mask\"],\n",
    "            dtype=torch.long,\n",
    "            device=DEVICE\n",
    "        ).unsqueeze(0)  # [1, T]\n",
    "\n",
    "        # Skip short sequences\n",
    "        if input_ids.size(1) <= CHUNK_SIZE:\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_tokens(input_ids, attention_mask, CHUNK_SIZE)\n",
    "\n",
    "        if len(chunks) < 2:\n",
    "            continue\n",
    "\n",
    "        loss = train_on_chunks(chunks, model, stm_trainer)\n",
    "\n",
    "        total_loss += loss\n",
    "        n_docs += 1\n",
    "\n",
    "    avg_loss = total_loss / max(n_docs, 1)\n",
    "    print(f\"Epoch {epoch} | STM loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ab0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"stm_state_dict\": stm.state_dict(),\n",
    "        \"stm_config\": {\n",
    "            \"hidden_size\": model.config.hidden_size,\n",
    "            \"num_layers\": 2,\n",
    "            \"num_heads\": 4\n",
    "        }\n",
    "    },\n",
    "    f\"{stm_dir}/stm_checkpoint.pt\"\n",
    ")\n",
    "torch.save(\n",
    "    stm_trainer.state_dict(),\n",
    "    f\"{stm_dir}/stm_trainer_checkpoint.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outline for LTM\n",
    "# Pass in summary tokens, get k latent vectors of size d (hidden state size)\n",
    "# Run concated original tokens with latent vectors through frozen stm model with decoding head to get next token\n",
    "# Train LTM only against predicted vs actual next token\n",
    "# Pseudocode\n",
    "# input_tokens -> stm_model -> summary tokens -> ltm_model -> latent vectors -> concat with input_tokens -> stm_model + decoding head -> next token prediction -> loss, backprop LTM only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a417a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongTermMemory(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        num_latents=8,\n",
    "        num_layers=4,\n",
    "        num_heads=4\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Learned latent tokens\n",
    "        self.latents = nn.Parameter(\n",
    "            torch.randn(1, num_latents, hidden_size)\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, summaries, attention_mask=None):\n",
    "        \"\"\"\n",
    "        summaries: [B, T, d]\n",
    "        returns:   [B, K, d]\n",
    "        \"\"\"\n",
    "\n",
    "        B = summaries.size(0)\n",
    "\n",
    "        latents = self.latents.expand(B, -1, -1)\n",
    "\n",
    "        # Concatenate: latents attend to summaries\n",
    "        x = torch.cat([latents, summaries], dim=1)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            latent_mask = torch.ones(\n",
    "                B,\n",
    "                latents.size(1),\n",
    "                device=attention_mask.device\n",
    "            )\n",
    "            attn_mask = torch.cat([latent_mask, attention_mask], dim=1)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        out = self.encoder(x, src_key_padding_mask=(attn_mask == 0 if attn_mask is not None else None))\n",
    "\n",
    "        # Return only latent outputs\n",
    "        return out[:, :latents.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61103069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_chunks_with_hiddens(chunks, stm_model):\n",
    "    \"\"\"\n",
    "    Encode chunks and return both summaries and hidden states\n",
    "    \n",
    "    Returns:\n",
    "        summaries: [num_chunks, d]\n",
    "        chunk_hiddens: List of [1, T_i, d]\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    chunk_hiddens = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chunk in chunks:\n",
    "            outputs = model(\n",
    "                input_ids=chunk[\"input_ids\"],\n",
    "                attention_mask=chunk[\"attention_mask\"]\n",
    "            )\n",
    "            \n",
    "            # Token-level hidden states\n",
    "            h = outputs.last_hidden_state  # [1, T, d]\n",
    "            \n",
    "            # Get summary via STM\n",
    "            summary = stm_model(\n",
    "                hidden_states=h,\n",
    "                attention_mask=chunk[\"attention_mask\"]\n",
    "            )\n",
    "            \n",
    "            summaries.append(summary.squeeze(0))  # [d]\n",
    "            chunk_hiddens.append(h)  # [1, T, d]\n",
    "    \n",
    "    summaries_stacked = torch.stack(summaries)  # [num_chunks, d]\n",
    "    return summaries_stacked, chunk_hiddens\n",
    "\n",
    "def train_ltm_step(\n",
    "    chunks,\n",
    "    summaries,  # [1, num_chunks, d]\n",
    "    chunk_hiddens,  # List of [1, T_i, d]\n",
    "    ltm,\n",
    "    stm_trainer,\n",
    "    optimizer,\n",
    "    DEVICE,\n",
    "    K,\n",
    "    N=3  # Number of refinement iterations\n",
    "):\n",
    "    \"\"\"\n",
    "    Train LTM with iterative latent refinement:\n",
    "    1. Get initial latents from LTM using context summaries\n",
    "    2. For N iterations:\n",
    "       - Concatenate chunk hidden states with current latents\n",
    "       - Pass through STM to get refined representation\n",
    "       - Pass refined representation through LTM to generate new latents\n",
    "    3. Use final latents concatenated with chunk hidden states to predict next token\n",
    "    \"\"\"\n",
    "    num_chunks = len(chunks)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i in range(num_chunks - 1):\n",
    "        # Get summaries from chunks 0 to i (context for LTM)\n",
    "        context_summaries = summaries[:, :i+1]  # [1, i+1, d]\n",
    "        \n",
    "        # Pass through LTM to get initial latent vectors\n",
    "        latents = ltm(context_summaries)  # [1, K, d]\n",
    "        \n",
    "        # Get hidden states of current chunk\n",
    "        chunk_h = chunk_hiddens[i]  # [1, T_i, d]\n",
    "        \n",
    "        # Iteratively refine latents N times\n",
    "        for _ in range(N):\n",
    "            # Concatenate chunk hidden states with current latents\n",
    "            combined = torch.cat([chunk_h, latents], dim=1)  # [1, T_i+K, d]\n",
    "            \n",
    "            # Create attention mask\n",
    "            attention_mask = torch.ones(combined.shape[:2], dtype=torch.long, device=DEVICE)\n",
    "            \n",
    "            # Pass through STM to get refined summary\n",
    "            refined_summary = stm_trainer.stm(combined, attention_mask)  # [1, d]\n",
    "            \n",
    "            # Use refined summary to generate new latents via LTM\n",
    "            refined_context = refined_summary.unsqueeze(1)  # [1, 1, d]\n",
    "            latents = ltm(refined_context)  # [1, K, d]\n",
    "        \n",
    "        # Final prediction: concatenate chunk_h with final latents\n",
    "        final_combined = torch.cat([chunk_h, latents], dim=1)  # [1, T_i+K, d]\n",
    "        attention_mask = torch.ones(final_combined.shape[:2], dtype=torch.long, device=DEVICE)\n",
    "        \n",
    "        # Get logits for next token prediction via stm_trainer head\n",
    "        logits = stm_trainer(final_combined, attention_mask)  # [1, vocab]\n",
    "        \n",
    "        # Target: first token of the next chunk\n",
    "        target = chunks[i+1][\"input_ids\"][:, 0]  # [1]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        \n",
    "        # Backprop (only updates LTM)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / max(num_chunks - 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a561b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2 # number of latents\n",
    "N = 3  # number of refinement iterations\n",
    "epochs = 10 \n",
    "JSONL_PATH = \"E:\\\\YuyangGPT\\\\dataset\\\\cleaned_data\\\\train_tokenized_discord_messages.jsonl\" # Tokenized training data\n",
    "ltm = LongTermMemory(\n",
    "    hidden_size=model.config.hidden_size,\n",
    "    num_latents=K,\n",
    "    num_layers=4,\n",
    "    num_heads=4\n",
    ").to(DEVICE)\n",
    "\n",
    "ltm_optimizer = torch.optim.AdamW(\n",
    "    ltm.parameters(),\n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    n_docs = 0\n",
    "\n",
    "    for sample in load_jsonl(JSONL_PATH):\n",
    "        # Convert lists → tensors\n",
    "        input_ids = torch.tensor(\n",
    "            sample[\"input_ids\"],\n",
    "            dtype=torch.long,\n",
    "            device=DEVICE\n",
    "        ).unsqueeze(0)  # [1, T]\n",
    "\n",
    "        attention_mask = torch.tensor(\n",
    "            sample[\"attention_mask\"],\n",
    "            dtype=torch.long,\n",
    "            device=DEVICE\n",
    "        ).unsqueeze(0)  # [1, T]\n",
    "\n",
    "        # Skip short sequences\n",
    "        if input_ids.size(1) <= CHUNK_SIZE:\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_tokens(input_ids, attention_mask, CHUNK_SIZE)\n",
    "\n",
    "        if len(chunks) < K + 1:\n",
    "            continue\n",
    "        \n",
    "        # Encode chunks to get summaries and hidden states\n",
    "        summaries, chunk_hiddens = encode_chunks_with_hiddens(chunks, stm)\n",
    "        summaries_batched = summaries.unsqueeze(0)  # [1, num_chunks, d]\n",
    "        \n",
    "        # Train LTM on this batch with iterative refinement\n",
    "        loss = train_ltm_step(\n",
    "            chunks,\n",
    "            summaries_batched,\n",
    "            chunk_hiddens,\n",
    "            ltm,\n",
    "            stm_trainer,\n",
    "            ltm_optimizer,\n",
    "            DEVICE,\n",
    "            K,\n",
    "            N  # Pass refinement iterations\n",
    "        )\n",
    "        \n",
    "        total_loss += loss\n",
    "        n_docs += 1\n",
    "\n",
    "    avg_loss = total_loss / max(n_docs, 1)\n",
    "    print(f\"Epoch {epoch} | LTM loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46720fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm_dir = \"E:\\YuyangGPT\\models\\ltms\\ltm_1\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"ltm_state_dict\": ltm.state_dict(),\n",
    "        \"ltm_config\": {\n",
    "            \"hidden_size\": model.config.hidden_size,\n",
    "            \"num_latents\": K,\n",
    "            \"num_layers\": 4,\n",
    "            \"num_heads\": 4\n",
    "        },\n",
    "        \"predictor_state_dict\": ltm_predictor.state_dict()\n",
    "    },\n",
    "    f\"{ltm_dir}/ltm_checkpoint.pt\"\n",
    ")\n",
    "\n",
    "torch.save(\n",
    "    ltm_predictor.state_dict(),\n",
    "    f\"{ltm_dir}/ltm_predictor_checkpoint.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9602eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Hello! I am YuyangGPT, a custom language model developed by Yuyang Hu, to mimic his style of writing and to increase his ability to be lazy\n",
    "\"\"\"\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d3cca0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ").to(DEVICE)\n",
    "\n",
    "# Initialize inputs\n",
    "current_input_ids = inputs[\"input_ids\"]\n",
    "current_attention_mask = inputs[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4aa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTM Latents raw tensors:  tensor([[[ 6.1346e-03,  2.0783e-01, -1.4621e-02,  1.2475e-02, -4.2895e-02,\n",
      "           2.4536e-02, -3.1636e-02, -3.5248e-02,  3.1469e-02, -7.7356e-02,\n",
      "           1.0164e-01,  5.6215e-03,  7.1762e-02,  6.4854e-02,  3.2184e-02,\n",
      "          -1.0968e-01,  2.3746e-02,  2.4515e-01, -2.3751e-01, -1.6380e-02,\n",
      "          -9.6214e-02,  9.9117e-03, -4.5560e-02,  2.1287e-01, -2.6197e-03,\n",
      "           8.4560e-02,  1.1555e-02,  3.6881e-02,  5.1513e-02, -2.3701e-02,\n",
      "          -2.5928e-02, -4.1905e-02,  1.5209e-01, -3.0275e-02, -1.8244e-04,\n",
      "          -6.3817e-03,  7.6464e-02, -1.4012e-02,  4.5082e+00, -1.0293e-01,\n",
      "           8.2999e-02, -9.9391e-05, -1.9487e-01,  5.5413e-02, -2.7280e+00,\n",
      "          -6.3853e-02, -1.3144e-01,  2.7343e+00,  1.4637e-02,  5.4035e-01,\n",
      "           1.8240e-02, -4.2589e-03, -1.1037e-01,  1.2690e-02,  6.7131e-02,\n",
      "           3.9765e-02, -1.2319e+01,  2.9649e-02, -6.6016e-02,  6.5117e-02,\n",
      "          -2.5257e-02, -1.6618e-02,  6.2852e-01,  4.8459e-02, -3.8034e-01,\n",
      "          -7.9692e-02,  4.6426e-01, -4.8783e-02,  5.4127e-02, -6.3189e-02,\n",
      "          -4.5003e-02, -3.0888e-02, -3.1748e-02, -1.6584e-02,  1.7400e-02,\n",
      "           5.4110e-01,  2.8810e-02,  8.6900e-02,  2.0120e-02,  1.8054e-02,\n",
      "          -4.7852e-02, -7.8087e-02,  1.1601e-01,  7.7408e-02, -2.6661e-02,\n",
      "          -4.9059e-01,  1.0728e+00,  1.2359e-02,  1.0723e-01, -1.1608e-02,\n",
      "           1.3430e-02, -4.2016e-01, -1.5114e-02, -1.5788e-02, -3.1903e-02,\n",
      "          -8.8825e-02,  2.6104e-02, -8.5004e-01,  2.9050e-02,  9.2714e-01,\n",
      "          -3.1676e-01,  2.8998e-02, -2.7815e-01,  8.7415e-02,  2.5060e-01,\n",
      "          -2.9200e-02, -1.0239e+00, -3.7811e-02, -3.6584e-01,  1.6011e-01,\n",
      "          -6.5375e-01, -3.8082e-02, -1.8702e-01,  1.3672e-02,  2.3120e-02,\n",
      "          -4.5244e-02, -7.2506e-02,  3.6322e-02, -4.6178e-02, -3.8548e-02,\n",
      "          -5.0811e-02, -6.6751e-02,  3.9738e-01, -1.6714e-03,  1.7600e-01,\n",
      "          -6.5601e-02, -1.9959e-02, -5.8041e-02,  2.7715e-02,  5.8511e-02,\n",
      "          -5.8563e-02, -1.7703e-02,  6.5178e-02,  3.8877e-02,  1.2924e-02,\n",
      "           7.1757e-02, -1.4303e-01, -5.3273e-01,  2.6580e-02, -2.7806e-01,\n",
      "          -1.2621e-01, -3.6171e-03, -1.7684e-01, -4.0857e-02, -2.4395e-01,\n",
      "          -7.1553e-02, -3.3011e+00, -5.7773e-03, -7.1387e-02, -1.5937e-01,\n",
      "          -2.0841e-02,  6.0966e-02, -6.1554e-02, -4.8686e-02, -1.4319e+00,\n",
      "          -1.7848e-02, -1.8936e-01,  4.4389e-01, -5.2420e-02, -3.8404e-03,\n",
      "          -6.5661e-02,  6.1291e-02,  3.9049e-02, -3.2800e-01, -7.7369e-02,\n",
      "          -4.1094e-02, -3.8349e-02, -4.5149e+00, -8.9767e-02,  3.5792e-01,\n",
      "           3.5916e-02,  2.4806e-02, -9.1053e-02, -4.9772e-02, -2.4669e-02,\n",
      "          -2.6170e-01, -6.9117e-03, -5.5626e-03,  3.9704e+00, -2.1334e-02,\n",
      "          -2.7196e-02, -2.8576e-01,  6.2555e+00, -1.1789e-01,  3.4952e-02,\n",
      "           2.2855e-02,  1.1309e+00,  3.6663e-02, -9.5317e-02, -5.5848e-02,\n",
      "          -1.9960e-02,  2.9337e-03,  5.8580e-01, -1.8674e-01,  7.1091e-02,\n",
      "          -7.1991e-02, -4.3766e-02,  1.7624e-03, -3.2951e-02,  4.9305e-03,\n",
      "           1.9150e-03, -2.3346e-02, -2.1632e-02,  1.7422e-01, -1.8014e-01,\n",
      "           6.6284e-04, -2.0106e-01, -2.3779e-02, -1.9916e-02,  5.4978e-02,\n",
      "           2.5259e-01, -4.6465e-02,  7.5789e-02,  1.7267e-02,  3.5247e-03,\n",
      "           3.5360e-02, -2.8663e-01, -2.6607e-01,  2.2185e+00, -5.3545e+00,\n",
      "          -1.0662e-01, -6.4605e-03, -9.0513e-01, -2.0814e-02, -2.9475e-02,\n",
      "           2.4596e-02,  2.4891e-02, -2.8911e-02,  1.4433e-01, -3.3445e-02,\n",
      "          -3.5555e-02, -4.8756e-02,  1.3032e-01, -1.3016e-02,  3.9325e-02,\n",
      "          -4.9091e-02, -1.4925e-02, -1.6447e-02,  1.0523e+00,  2.9915e-01,\n",
      "          -8.8389e-02,  3.6897e-01, -1.4047e-01,  4.3094e-02,  3.7562e-03,\n",
      "          -1.1891e-02, -2.5003e-02, -5.6026e-02, -3.8348e-03,  2.1271e-01,\n",
      "           1.4222e-01, -3.7245e-02, -6.4313e-01, -5.8688e-02, -5.4286e-02,\n",
      "          -7.0363e-02,  4.7553e-02, -1.4576e-01,  3.2254e-02, -2.9316e-02,\n",
      "          -2.5193e-02, -3.9765e-02, -4.7682e-02, -7.3594e-03,  1.6947e-03,\n",
      "          -2.4201e-01,  3.5578e-01, -5.4263e-03, -1.3091e-01, -1.9664e-02,\n",
      "           1.4498e+01, -4.0177e-02, -3.9762e-03,  4.6010e-01,  9.5746e-02,\n",
      "           1.6833e-01, -1.3984e-02, -1.9113e-02,  4.2407e-02,  1.5050e-02,\n",
      "           9.2409e-02,  6.8754e-01, -4.9008e-01,  6.0534e-02,  1.0869e-02,\n",
      "          -3.9876e-02, -2.2253e-01, -3.0035e-01, -1.3956e-01,  3.5773e-03,\n",
      "           4.8950e-03, -3.5010e-01, -1.4962e-02, -7.8741e-01, -1.6619e-02,\n",
      "          -2.5670e-01, -3.0729e-01,  1.7659e-01, -3.4796e-02,  2.8870e-02,\n",
      "           2.8351e-02, -5.2195e-02, -3.2954e-02, -1.7117e-02, -5.7602e-02,\n",
      "          -7.5177e-02,  3.4773e-02, -3.7238e-02, -3.5718e-02,  1.6256e-01,\n",
      "           1.4279e-02,  2.1276e-02,  5.5472e-02, -5.6848e-02,  5.5830e-03,\n",
      "          -5.0266e-02, -2.0255e-01, -1.8193e-01, -5.8497e-02, -1.2686e-01,\n",
      "          -1.2749e-01, -7.3906e-02,  5.4463e-02, -3.1286e-01,  1.9978e-02,\n",
      "           1.0368e-01, -1.6869e-01, -6.0820e-02,  7.1110e-02, -2.1490e-02,\n",
      "          -6.3622e-02, -5.0268e-02, -2.6213e-02,  5.5636e-02,  1.1962e-02,\n",
      "          -2.0876e-01, -3.0856e-01, -2.4905e-02, -6.7609e-02,  2.2231e-02,\n",
      "          -3.2356e-01, -4.3297e-01,  4.6353e-02, -2.4357e-02, -4.4273e-02,\n",
      "          -2.0031e-02,  2.5042e-02,  3.2786e-02, -2.4470e-01,  1.2911e-01,\n",
      "          -7.3012e-03, -3.6062e-02, -1.0416e-01, -2.2759e-03, -1.8104e-02,\n",
      "           1.1956e-01,  5.1259e-02, -6.2128e-02,  1.6105e-02, -1.1823e-02,\n",
      "           3.6922e-02, -6.8595e-02,  8.2100e-02, -1.8022e-02, -3.5804e-04,\n",
      "          -1.6096e-02, -6.5053e-03,  1.2504e-02, -9.5516e-03, -7.0426e-02,\n",
      "           2.2984e+00,  1.6814e-01, -8.6994e-01, -4.4843e-02, -4.9465e-03,\n",
      "           1.1646e-02,  8.1840e-02,  8.3102e-01, -1.3267e-01, -2.9702e-02,\n",
      "           3.2743e-02,  3.4598e-02,  1.7864e-02,  2.6365e-01],\n",
      "         [ 6.1468e-03,  2.0782e-01, -1.4600e-02,  1.2456e-02, -4.2914e-02,\n",
      "           2.4511e-02, -3.1637e-02, -3.5265e-02,  3.1445e-02, -7.7369e-02,\n",
      "           1.0156e-01,  5.6297e-03,  7.1754e-02,  6.4873e-02,  3.2078e-02,\n",
      "          -1.0974e-01,  2.3794e-02,  2.4521e-01, -2.3748e-01, -1.6248e-02,\n",
      "          -9.6124e-02,  9.9440e-03, -4.5594e-02,  2.1292e-01, -2.7859e-03,\n",
      "           8.4521e-02,  1.1538e-02,  3.6924e-02,  5.1462e-02, -2.3648e-02,\n",
      "          -2.5816e-02, -4.1928e-02,  1.5218e-01, -3.0352e-02, -1.8706e-04,\n",
      "          -6.3657e-03,  7.6420e-02, -1.4041e-02,  4.5082e+00, -1.0295e-01,\n",
      "           8.3026e-02, -1.7381e-04, -1.9489e-01,  5.5451e-02, -2.7279e+00,\n",
      "          -6.3783e-02, -1.3141e-01,  2.7343e+00,  1.4641e-02,  5.4038e-01,\n",
      "           1.8231e-02, -4.2444e-03, -1.1036e-01,  1.2693e-02,  6.7168e-02,\n",
      "           3.9687e-02, -1.2319e+01,  2.9675e-02, -6.6029e-02,  6.5240e-02,\n",
      "          -2.5186e-02, -1.6667e-02,  6.2851e-01,  4.8441e-02, -3.8035e-01,\n",
      "          -7.9647e-02,  4.6424e-01, -4.8650e-02,  5.4161e-02, -6.3287e-02,\n",
      "          -4.5031e-02, -3.0922e-02, -3.1682e-02, -1.6509e-02,  1.7416e-02,\n",
      "           5.4108e-01,  2.8785e-02,  8.6867e-02,  2.0134e-02,  1.8041e-02,\n",
      "          -4.7916e-02, -7.8169e-02,  1.1603e-01,  7.7442e-02, -2.6641e-02,\n",
      "          -4.9061e-01,  1.0728e+00,  1.2385e-02,  1.0726e-01, -1.1545e-02,\n",
      "           1.3426e-02, -4.2018e-01, -1.5119e-02, -1.5769e-02, -3.1919e-02,\n",
      "          -8.8853e-02,  2.6113e-02, -8.4999e-01,  2.9052e-02,  9.2715e-01,\n",
      "          -3.1673e-01,  2.8984e-02, -2.7816e-01,  8.7452e-02,  2.5043e-01,\n",
      "          -2.9158e-02, -1.0241e+00, -3.7891e-02, -3.6589e-01,  1.6019e-01,\n",
      "          -6.5377e-01, -3.8100e-02, -1.8695e-01,  1.3635e-02,  2.3082e-02,\n",
      "          -4.5272e-02, -7.2554e-02,  3.6307e-02, -4.6201e-02, -3.8526e-02,\n",
      "          -5.0802e-02, -6.6830e-02,  3.9749e-01, -1.7177e-03,  1.7603e-01,\n",
      "          -6.5562e-02, -2.0013e-02, -5.8045e-02,  2.7690e-02,  5.8504e-02,\n",
      "          -5.8565e-02, -1.7671e-02,  6.5221e-02,  3.8898e-02,  1.2925e-02,\n",
      "           7.1774e-02, -1.4301e-01, -5.3279e-01,  2.6592e-02, -2.7801e-01,\n",
      "          -1.2630e-01, -3.5144e-03, -1.7697e-01, -4.0864e-02, -2.4389e-01,\n",
      "          -7.1503e-02, -3.3012e+00, -5.8104e-03, -7.1399e-02, -1.5940e-01,\n",
      "          -2.0829e-02,  6.0991e-02, -6.1555e-02, -4.8694e-02, -1.4319e+00,\n",
      "          -1.7807e-02, -1.8935e-01,  4.4389e-01, -5.2391e-02, -3.8201e-03,\n",
      "          -6.5671e-02,  6.1340e-02,  3.9000e-02, -3.2803e-01, -7.7347e-02,\n",
      "          -4.1141e-02, -3.8313e-02, -4.5149e+00, -8.9678e-02,  3.5794e-01,\n",
      "           3.6026e-02,  2.4797e-02, -9.0942e-02, -4.9829e-02, -2.4735e-02,\n",
      "          -2.6165e-01, -6.9075e-03, -5.5624e-03,  3.9705e+00, -2.1269e-02,\n",
      "          -2.7260e-02, -2.8568e-01,  6.2556e+00, -1.1796e-01,  3.4980e-02,\n",
      "           2.2834e-02,  1.1310e+00,  3.6639e-02, -9.5352e-02, -5.5836e-02,\n",
      "          -1.9948e-02,  2.9384e-03,  5.8586e-01, -1.8688e-01,  7.1094e-02,\n",
      "          -7.1969e-02, -4.3792e-02,  1.6916e-03, -3.2914e-02,  4.9739e-03,\n",
      "           1.8015e-03, -2.3401e-02, -2.1658e-02,  1.7431e-01, -1.8014e-01,\n",
      "           7.1807e-04, -2.0108e-01, -2.3718e-02, -1.9937e-02,  5.4905e-02,\n",
      "           2.5267e-01, -4.6513e-02,  7.5762e-02,  1.7302e-02,  3.5399e-03,\n",
      "           3.5333e-02, -2.8663e-01, -2.6613e-01,  2.2185e+00, -5.3545e+00,\n",
      "          -1.0660e-01, -6.3782e-03, -9.0513e-01, -2.0755e-02, -2.9502e-02,\n",
      "           2.4585e-02,  2.4796e-02, -2.8957e-02,  1.4434e-01, -3.3434e-02,\n",
      "          -3.5477e-02, -4.8724e-02,  1.3037e-01, -1.3116e-02,  3.9349e-02,\n",
      "          -4.9120e-02, -1.4866e-02, -1.6424e-02,  1.0523e+00,  2.9917e-01,\n",
      "          -8.8375e-02,  3.6902e-01, -1.4050e-01,  4.3117e-02,  3.6626e-03,\n",
      "          -1.1897e-02, -2.4977e-02, -5.6011e-02, -3.7154e-03,  2.1269e-01,\n",
      "           1.4216e-01, -3.7231e-02, -6.4318e-01, -5.8640e-02, -5.4216e-02,\n",
      "          -7.0312e-02,  4.7614e-02, -1.4581e-01,  3.2249e-02, -2.9278e-02,\n",
      "          -2.5202e-02, -3.9760e-02, -4.7661e-02, -7.3638e-03,  1.6721e-03,\n",
      "          -2.4203e-01,  3.5580e-01, -5.4535e-03, -1.3091e-01, -1.9617e-02,\n",
      "           1.4498e+01, -4.0110e-02, -4.0201e-03,  4.6019e-01,  9.5662e-02,\n",
      "           1.6828e-01, -1.4045e-02, -1.9117e-02,  4.2418e-02,  1.5131e-02,\n",
      "           9.2359e-02,  6.8755e-01, -4.9005e-01,  6.0489e-02,  1.0816e-02,\n",
      "          -3.9847e-02, -2.2262e-01, -3.0031e-01, -1.3953e-01,  3.6759e-03,\n",
      "           4.7981e-03, -3.5026e-01, -1.4991e-02, -7.8750e-01, -1.6610e-02,\n",
      "          -2.5669e-01, -3.0725e-01,  1.7657e-01, -3.4793e-02,  2.8905e-02,\n",
      "           2.8343e-02, -5.2221e-02, -3.2966e-02, -1.7136e-02, -5.7549e-02,\n",
      "          -7.5161e-02,  3.4683e-02, -3.7261e-02, -3.5694e-02,  1.6257e-01,\n",
      "           1.4266e-02,  2.1301e-02,  5.5494e-02, -5.6839e-02,  5.5585e-03,\n",
      "          -5.0235e-02, -2.0249e-01, -1.8198e-01, -5.8496e-02, -1.2692e-01,\n",
      "          -1.2747e-01, -7.3861e-02,  5.4484e-02, -3.1288e-01,  1.9956e-02,\n",
      "           1.0368e-01, -1.6874e-01, -6.0831e-02,  7.1079e-02, -2.1470e-02,\n",
      "          -6.3554e-02, -5.0221e-02, -2.6202e-02,  5.5598e-02,  1.2043e-02,\n",
      "          -2.0883e-01, -3.0858e-01, -2.4973e-02, -6.7640e-02,  2.2142e-02,\n",
      "          -3.2347e-01, -4.3291e-01,  4.6382e-02, -2.4392e-02, -4.4303e-02,\n",
      "          -2.0011e-02,  2.4991e-02,  3.2751e-02, -2.4476e-01,  1.2904e-01,\n",
      "          -7.2952e-03, -3.6023e-02, -1.0427e-01, -2.2856e-03, -1.8091e-02,\n",
      "           1.1951e-01,  5.1281e-02, -6.2075e-02,  1.6080e-02, -1.1886e-02,\n",
      "           3.6985e-02, -6.8621e-02,  8.2032e-02, -1.8035e-02, -3.6544e-04,\n",
      "          -1.6235e-02, -6.5079e-03,  1.2422e-02, -9.6394e-03, -7.0355e-02,\n",
      "           2.2985e+00,  1.6816e-01, -8.7010e-01, -4.4799e-02, -4.9466e-03,\n",
      "           1.1749e-02,  8.1783e-02,  8.3102e-01, -1.3265e-01, -2.9739e-02,\n",
      "           3.2797e-02,  3.4553e-02,  1.7841e-02,  2.6363e-01]]],\n",
      "       device='cuda:0')\n",
      "LTM latents shape: torch.Size([1, 2, 384])\n",
      "LTM generated tokens: / /\n",
      "New input length: 37\n",
      "Full text: hello! i am yuyanggpt, a custom language model developed by yuyang hu, to mimic his style of writing and to increase his ability to be lazy / /\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_tokens(inputs[\"input_ids\"], inputs[\"attention_mask\"], chunk_size=128)\n",
    "\n",
    "# Encode chunks to get summaries and hidden states\n",
    "summaries, chunk_hiddens = encode_chunks_with_hiddens(chunks, stm)\n",
    "summaries_batched = summaries.unsqueeze(0)  # [1, num_chunks, d]\n",
    "\n",
    "# Pass summaries through LTM to generate latents\n",
    "with torch.no_grad():\n",
    "    ltm_latents = ltm(summaries_batched)  # [1, K, d]\n",
    "\n",
    "print(\"LTM Latents shape:\", ltm_latents.shape)\n",
    "\n",
    "# Concatenate latents with the last chunk's hidden states\n",
    "last_chunk_hidden = chunk_hiddens[-1]  # [1, T, d]\n",
    "combined = torch.cat([last_chunk_hidden, ltm_latents], dim=1)  # [1, T+K, d]\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = torch.ones(combined.shape[:2], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "# Pass through stm_trainer to predict next token\n",
    "ltm_logits = stm_trainer(combined, attention_mask)  # [1, vocab]\n",
    "ltm_tokens = torch.argmax(ltm_logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "decoded_text = tokenizer.decode(\n",
    "    ltm_tokens[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"LTM generated tokens:\", decoded_text)\n",
    "\n",
    "# Append LTM tokens to current input\n",
    "current_input_ids = torch.cat(\n",
    "    [inputs[\"input_ids\"], ltm_tokens],\n",
    "    dim=1\n",
    ")  # [1, T + 1]\n",
    "\n",
    "current_attention_mask = torch.ones_like(current_input_ids)\n",
    "print(f\"New input length: {current_input_ids.shape[1]}\")\n",
    "print(f\"Full text: {tokenizer.decode(current_input_ids[0], skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d51ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries shape: torch.Size([1, 1, 384])\n",
      "LTM latents shape: torch.Size([1, 2, 384])\n",
      "LTM generated tokens: / /\n",
      "New input length: 15\n",
      "Full text: this is a test of yuyanggpt v1 / /\n",
      "Summaries shape: torch.Size([1, 1, 384])\n",
      "LTM latents shape: torch.Size([1, 2, 384])\n",
      "LTM generated tokens: / /\n",
      "New input length: 15\n",
      "Full text: this is a test of yuyanggpt v1 / /\n",
      "Summaries shape: torch.Size([1, 1, 384])\n",
      "LTM latents shape: torch.Size([1, 2, 384])\n",
      "LTM generated tokens: / /\n",
      "New input length: 15\n",
      "Full text: this is a test of yuyanggpt v1 / /\n",
      "Summaries shape: torch.Size([1, 1, 384])\n",
      "LTM latents shape: torch.Size([1, 2, 384])\n",
      "LTM generated tokens: / /\n",
      "New input length: 15\n",
      "Full text: this is a test of yuyanggpt v1 / /\n",
      "Summaries shape: torch.Size([1, 1, 384])\n",
      "LTM latents shape: torch.Size([1, 2, 384])\n",
      "LTM generated tokens: / /\n",
      "New input length: 15\n",
      "Full text: this is a test of yuyanggpt v1 / /\n",
      "Summaries shape: torch.Size([1, 1, 384])\n",
      "LTM latents shape: torch.Size([1, 2, 384])\n",
      "LTM generated tokens: / /\n",
      "New input length: 15\n",
      "Full text: this is a test of yuyanggpt v1 / /\n",
      "Summaries shape: torch.Size([1, 1, 384])\n",
      "LTM latents shape: torch.Size([1, 2, 384])\n",
      "LTM generated tokens: / /\n",
      "New input length: 15\n",
      "Full text: this is a test of yuyanggpt v1 / /\n",
      "Summaries shape: torch.Size([1, 1, 384])\n",
      "LTM latents shape: torch.Size([1, 2, 384])\n",
      "LTM generated tokens: / /\n",
      "New input length: 15\n",
      "Full text: this is a test of yuyanggpt v1 / /\n",
      "Summaries shape: torch.Size([1, 1, 384])\n",
      "LTM latents shape: torch.Size([1, 2, 384])\n",
      "LTM generated tokens: / /\n",
      "New input length: 15\n",
      "Full text: this is a test of yuyanggpt v1 / /\n",
      "\n",
      "===== FINAL OUTPUT =====\n",
      "Final sequence length: 15\n",
      "Final text: this is a test of yuyanggpt v1 / /\n"
     ]
    }
   ],
   "source": [
    "# ---- STEPS 1-N: Refine with new chunks ----\n",
    "for step in range(1, N):\n",
    "    chunks = chunk_tokens(current_input_ids, current_attention_mask, chunk_size=128)\n",
    "    \n",
    "    # Encode chunks to get summaries and hidden states\n",
    "    summaries, chunk_hiddens = encode_chunks_with_hiddens(chunks, stm)\n",
    "    summaries_batched = summaries.unsqueeze(0)  # [1, num_chunks, d]\n",
    "\n",
    "    print(\"Summaries shape:\", summaries_batched.shape)\n",
    "\n",
    "    # Pass summaries through LTM to generate latents\n",
    "    with torch.no_grad():\n",
    "        ltm_latents = ltm(summaries_batched)  # [1, K, d]\n",
    "\n",
    "    print(\"LTM latents shape:\", ltm_latents.shape)\n",
    "\n",
    "    # Concatenate latents with the last chunk's hidden states\n",
    "    last_chunk_hidden = chunk_hiddens[-1]  # [1, T, d]\n",
    "    combined = torch.cat([last_chunk_hidden, ltm_latents], dim=1)  # [1, T+K, d]\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = torch.ones(combined.shape[:2], dtype=torch.long, device=DEVICE)\n",
    "    \n",
    "    # Predict next token\n",
    "    ltm_logits = stm_trainer(combined, attention_mask)  # [1, vocab]\n",
    "    ltm_tokens = torch.argmax(ltm_logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "    decoded_text = tokenizer.decode(\n",
    "        ltm_tokens[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(\"LTM generated tokens:\", decoded_text)\n",
    "\n",
    "    # Append new token to current input\n",
    "    current_input_ids = torch.cat(\n",
    "        [current_input_ids, ltm_tokens],\n",
    "        dim=1\n",
    "    )  # [1, T + 1]\n",
    "\n",
    "    current_attention_mask = torch.ones_like(current_input_ids)\n",
    "\n",
    "    print(f\"New input length: {current_input_ids.shape[1]}\")\n",
    "    print(f\"Full text: {tokenizer.decode(current_input_ids[0], skip_special_tokens=True)}\")\n",
    "\n",
    "print(\"\\n===== FINAL OUTPUT =====\")\n",
    "print(f\"Final sequence length: {current_input_ids.shape[1]}\")\n",
    "print(f\"Final text: {tokenizer.decode(current_input_ids[0], skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9a7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
